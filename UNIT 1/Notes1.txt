What is Algorithm?
    An algorithm is a set of unambiguous instructions that take an input, 
    process it, and produce an output within a finite amount of time.

Difference Between Algorithm & Program:
    > Algorithm: Abstract logic (language-independent)
    > Program: Implementation of an algorithm in a programming language.
      Example:
          > Algorithm : ‚ÄúAdd A and B‚Äù (concept).
          > Program : sum = a + b; (C++ code). 

Every software ‚Äî from calculators to AI systems ‚Äî is based on algorithms.
Example:
    > Search bar ‚Üí String Matching Algorithms (KMP, Rabin-Karp).
    > E-commerce product sorting ‚Üí Sorting Algorithms (QuickSort, MergeSort).          

Good algorithms reduce time complexity and space usage.
Example:
  Sorting 1 million numbers:
    > Bubble Sort: ~10¬π¬≤ comparisons ‚Üí hours.
    > Merge Sort: ~2√ó10‚Å∑ comparisons ‚Üí seconds.

Real World Examples:    

| Domain         | Algorithm Example         | Role                    |
| -------------- | ------------------------- | ----------------------- |
| Search Engines | PageRank                  | Ranks web pages         |
| Maps           | Dijkstra‚Äôs Algorithm      | Finds shortest path     |
| Banking        | RSA Encryption            | Secure transactions     |
| E-commerce     | QuickSort + Binary Search | Sorting & searching     |
| AI/ML          | Backpropagation           | Neural network training |

Problem Type:
    1.Sorting 
    2.Searching   
    3.Optimization
    4.Graph Problem
    5.String Problem
    6.Numerical
    7.Geometric
    8.Combinatorial
    9.Divide and Conquer
    10.Dynamic Programming.

What is Algorithmic Efficiency?
    Algorithmic efficiency measures how well an algorithm uses resources when solving a problem, mainly:
        1.Time: How long the algorithm takes to run (running time)
                It‚Äôs usually expressed in terms of the number of basic operations or steps the algorithm performs.
        2.Space: How much memory it consumes (memory usage)

Types of Input Affecting Efficiency:
    1.Best Case: Input for which algorithm performs fastest
    2.Worst Case: Input for which algorithm performs slowest (important for guarantees)
    3.Average Case: Expected time over all inputs

 Asymptotic Notations:
   1.Big-O Notation O(g(n))     -> Upper bound (worst case)    -> f(n)‚â§c‚ãÖg(n) for some constant c and large n.
   2.Big-Omega Notation Œ©(g(n)) -> Lower bound (best case)	   -> f(n)‚â•c‚ãÖg(n) for some constant c, and large n.
   3.Big-Theta Notation Œò(g(n)) -> Tight bound (average case)  ->(n) is both O(g(n)) and Œ©(g(n)).

Non-recursive algorithms: Direct counting of operations
Recursive algorithms: Solving recurrence relations

Analysis of Non-Recursive Algorithms:
Steps:
    1.Identify basic operation
            The most important step the algorithm repeats (e.g., comparison, addition).
    2.Count frequency of basic operation
            How many times it executes in terms of n.
    3.Express as function of ùëõ
            Get T(n), the running time.
    4.Find asymptotic notation
            Simplify to O, Œ©,or Œò.
Example:                                   
    sum = 0;                                    
    for (i = 0; i < n; i++)
        sum += arr[i];

Analysis of Recursive Algorithms:
Steps:
    1.Form Recurrence Relation
        Write T(n) in terms of T(subproblem¬†size).
    2.Solve Recurrence
     We solve using:
        Substitution method
        Recursion tree
        Master theorem (quick formula for divide-and-conquer)
 
String Algorithms: Na√Øve String Matching Algorithm.
Example:
T = "AABAACAADAABAABA"
P = "AABA"
We want to find all positions where "AABA" occurs in T.

Naive String Matching Algorithm:
    1.Check the pattern at every possible position in the text.
    2.Compare characters of pattern P with substring of text T one by one.
    3.If all characters match ‚Üí pattern found.
    4.Otherwise ‚Üí shift by 1 and repeat.

Pseudocode:
NaivePatternSearch(T, P):
    n = length(T)
    m = length(P)
    for i = 0 to n - m:
        match = true
        for j = 0 to m - 1:
            if T[i + j] != P[j]:
                match = false
                break
        if match:
            print("Pattern found at index", i)

Example:
T = "AABAACAADAABAABA", P = "AABA"
i = 0 ‚Üí matches (output: index 0)
i = 1 ‚Üí mismatch (B vs A)
i = 2 ‚Üí mismatch
i = 3 ‚Üí mismatch
i = 9 ‚Üí matches (output: index 9)
i = 12 ‚Üí matches (output: index 12)
Output: Pattern found at indices 0, 9, 12            

Time Complexity:
    Worst case:O((n-m+1)m) ‚âà O(nm)
    Best case:O(n)
    Space complexity:O(1)
Space complexity:O(1)(no extra space needed)

Advantages:
    > Simple to implement
    > No preprocessing required

Disadvantages:
    > Inefficient for large text and patterns
    > Always shifts by 1, even when more can be skipped
    > Redundant Comparisons

Rabin-Karp Algorithm:
    The Rabin-Karp algorithm improves over the Na√Øve approach by using
    hashing to quickly compare substrings of the text with the pattern.
  Instead of comparing each character one-by-one, it:
    > Computes a hash value for the pattern.
    > Computes hash values for each substring of the text of the same length.
    > If hash values match ‚Üí do a direct character comparison
     (to confirm, avoiding false matches due to hash collisions).

Working process:
1.Preprocess:
    Let m = pattern length, n = text length.
    Compute hash of the pattern P.
    Compute hash of the first m characters of the text T.     

2.Slide the pattern over the text:
    At each position, compare the hash of the text window with the pattern‚Äôs hash.
    If hashes match ‚Üí compare actual characters (to avoid false positives).
    Update the hash for the next window using rolling hash.

3. Rolling Hash Concept
Instead of recomputing the hash from scratch for each window:
Remove the effect of the first character.
Add the effect of the new character.
This makes hash update O(1).     

Example(base d,mod q):
       hash(next)=(d(hash(current)-T[i].h)+T[i+m]) mod q
        where,
            h = d^m-1 mod q
            d = number of possible char
            q = a prime number(to  reduce collision)

Pseudocode:
RabinKarp(T, P, d, q):
    n = length(T)
    m = length(P)
    h = pow(d, m-1) % q
    p_hash = 0
    t_hash = 0

    // Precompute hash for pattern and first window
    for i in 0 to m-1:
        p_hash = (d * p_hash + P[i]) % q
        t_hash = (d * t_hash + T[i]) % q

    for i in 0 to n - m:
        if p_hash == t_hash:
            if T[i : i+m] == P:
                print("Pattern found at index", i)
        if i < n - m:
            t_hash = (d * (t_hash - T[i] * h) + T[i+m]) % q
            if t_hash < 0:
                t_hash += q

Example:
Example
T = "GEEKS FOR GEEKS", P = "GEEK", d = 256, q = 101
Compute pattern hash and first window hash.
Slide across text, updating hash in O(1).
When hashes match ‚Üí verify with direct comparison.

Complexity Analysis:
Average Case: O(n+m)
Worst Case:O(nm)
Space Complexity:O(1).

Advantages:
    > Fast for multiple pattern searches in the same text.
    > Efficient average-case performance.
    > Rolling hash avoids re-computing from scratch.

Disadvantage:
    > Hash collisions possible ‚Üí may cause false matches.
    > Worst-case performance same as Na√ØveO(nm).
    > Choosing a good modulus q is important to reduce collisions.

Growth Rate:
    How quickly the running time (or memory usage) of an algorithm 
    increases when the input size increases. 

|   Complexity   |   Name            |      Example                                 |   Growth Behavior                               |
| -------------- | ----------------- | -------------------------------------------- | ----------------------------------------------- |
|     O(1)       | Constant Time     | Accessing an array element by index          | Always takes same time regardless of input size |
|     O(log n)   | Logarithmic Time  | Binary Search                                | Grows slowly even for large inputs              |
|     O(n)       | Linear Time       | Traversing an array                          | Time grows directly with input size             |
|     O(n log n) | Linearithmic Time | Merge Sort, Quick Sort (avg case)            | Slightly slower than linear, but still good     |
|     O(n¬≤)      | Quadratic Time    | Bubble Sort, Selection Sort                  | Slow for large inputs                           |
|     O(2‚Åø)      | Exponential Time  | Solving Towers of Hanoi, Brute-force subsets | Becomes impossible for even medium inputs       |
|     O(n!)      | Factorial Time    | Traveling Salesman (brute force)             | Completely impractical except for tiny inputs   |

What is a Recurrence Relation?
    A recurrence relation is an equation that defines a problem in terms
    of its smaller subproblems.It tells us how the solution grows step by step.

Why We Use Recurrence?
    > To analyze recursive algorithms like Merge Sort, Binary Search, Tower of Hanoi, etc.
    > Helps find exact or approximate time complexity.

Formula:
    T(n) = a.T(n/b) + f(n)    

Methods to solve recurrence relations:
    > Substitution Method
    > Recursion Tree Method
    > Master Theorem

|        Method           |                 When to Use                        |                  Advantages                    |                          Disadvantages                                 |                          Example                           |
| ----------------------- | -------------------------------------------------- | ---------------------------------------------- | ---------------------------------------------------------------------- | ---------------------------------------------------------- |
|   Substitution Method   | When you can guess the solution form.              | Very flexible, works for many problems.        | Needs a correct guess before proof; guessing can be tricky.            | $T(n) = T(n-1) + 1 \Rightarrow O(n)$                       |
|    Recursion Tree       | When you want to visualize the work at each level. | Gives a clear picture; good for understanding. | Can be messy for complex recurrences.                                  | Merge Sort: $T(n) = 2T(n/2) + O(n) \Rightarrow O(n\log n)$ |
|    Master Theorem       | When the recurrence fits $T(n) = aT(n/b) + f(n)$.  | Quick and formula-based.                       | Cannot handle all forms (e.g., variable split, non-polynomial $f(n)$). | Merge Sort, Binary Search                                  |

Knuth‚ÄìMorris‚ÄìPratt (KMP) string matching algorithm:
Problem KMP Solves:
Given:
A text T of length n
A pattern P of length m
Goal:
Find all occurrences of P in T efficiently.

Why KMP is Better?
Na√Øve approach: For each position in T, check the pattern character-by-character.
Time complexity: O(mn) in worst case.

KMP: Avoids re-checking characters in T that we already know match.
Time complexity: O(n+m).

Main Idea:
KMP uses a prefix function (also called LPS = Longest Prefix which is also a Suffix) to skip unnecessary comparisons.

LPS Array:
For each index i in the pattern, LPS[i] stores the length of the longest prefix of P which is also a suffix of P[0...i].
Helps decide where to jump in the pattern when a mismatch happens.

Example:
Pattern: "ABABAC"
LPS array: [0, 0, 1, 2, 3, 0]

LPS Construction
Steps:
Start with len = 0 (length of the current matching prefix) and i = 1.
If P[i] == P[len], increment both and set LPS[i] = len.
If mismatch and len > 0, set len = LPS[len-1].
If mismatch and len = 0, set LPS[i] = 0 and move i forward.

Pattern Matching Using LPS:
Start scanning T and P from the beginning.
If characters match, move both pointers.
If j == m (pattern matched):
Record the starting index
Set j = LPS[j-1] to continue searching.
If mismatch:
If j > 0, set j = LPS[j-1].
Else, move i forward.

5. Complexity
LPS computation: O(m)
Pattern matching: O(n)
Total: O(n+m)

6. Example
Text: "ABABDABACDABABCABAB"
Pattern: "ABABCABAB"

Steps:
Compute LPS ‚Üí [0,0,1,2,0,1,2,3,4]
Scan text and jump using LPS on mismatches
Matches found at indices: 10

7. Pseudocode

void computeLPS(string pat, int m, int lps[]) {
    int len = 0, i = 1;
    lps[0] = 0;
    while (i < m) {
        if (pat[i] == pat[len]) {
            lps[i++] = ++len;
        } else {
            if (len != 0)
                len = lps[len - 1];
            else
                lps[i++] = 0;
        }
    }
}

void KMPSearch(string pat, string txt) {
    int m = pat.size(), n = txt.size();
    int lps[m];
    computeLPS(pat, m, lps);

    int i = 0, j = 0;
    while (i < n) {
        if (pat[j] == txt[i]) { i++; j++; }
        if (j == m) {
            cout << "Pattern found at index " << i - j << endl;
            j = lps[j - 1];
        } else if (i < n && pat[j] != txt[i]) {
            if (j != 0) j = lps[j - 1];
            else i++;
        }
    }
}


Manacher‚Äôs Algorithm:
Purpose:
Find the longest palindromic substring in a string in O(n) time.

Why not brute force?
Brute force: Check all substrings ‚Üí O(n¬≥) time (or O(n¬≤) with better checking).
Manacher‚Äôs: Smartly reuses palindrome info to solve in O(n).

Core Idea:
Transform the string to handle odd and even length palindromes uniformly.

Example: "abba" ‚Üí #a#b#b#a# (insert # between each char and at ends).
Keep track of:
C ‚Üí center of current palindrome
R ‚Üí right boundary of current palindrome
P[i] ‚Üí radius (half-length) of palindrome centered at position i

Mirror concept: If we already know a palindrome around C,
the palindrome at a symmetric position i' (mirror of i) can help reduce checks.

Expand: Try to expand palindrome centered at i beyond known info.

Steps:
Transform input ‚Üí T.
Initialize P[] = 0 for all positions.
For each i in T:
Find mirror index ‚Üí mirror = 2*C - i.
If i is inside the current right boundary R,
set P[i] = min(R - i, P[mirror]).
Expand around i while characters match.
If expanded palindrome goes beyond R,
update C and R.
Find the maximum P[i] ‚Üí gives the longest palindrome in T.
Convert back to original string indexing.

Example:
String: "abba"
Transform: #a#b#b#a#
Iterations will find longest palindrome #a#b#b#a# with radius 4.
Convert ‚Üí "abba"

Complexity:
Time: O(n)
Space: O(n) (due to transformed array and P[]).

Drawbacks:
    > Implementation is tricky for beginners.
    > Not as intuitive as KMP or Rabin‚ÄìKarp.

