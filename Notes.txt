What is Algorithm?
    An algorithm is a set of unambiguous instructions that take an input, 
    process it, and produce an output within a finite amount of time.

Difference Between Algorithm & Program:
    > Algorithm: Abstract logic (language-independent)
    > Program: Implementation of an algorithm in a programming language.
      Example:
          > Algorithm : ‚ÄúAdd A and B‚Äù (concept).
          > Program : sum = a + b; (C++ code). 

Every software ‚Äî from calculators to AI systems ‚Äî is based on algorithms.
Example:
    > Search bar ‚Üí String Matching Algorithms (KMP, Rabin-Karp).
    > E-commerce product sorting ‚Üí Sorting Algorithms (QuickSort, MergeSort).          

Good algorithms reduce time complexity and space usage.
Example:
  Sorting 1 million numbers:
    > Bubble Sort: ~10¬π¬≤ comparisons ‚Üí hours.
    > Merge Sort: ~2√ó10‚Å∑ comparisons ‚Üí seconds.

Real World Examples:    

| Domain         | Algorithm Example         | Role                    |
| -------------- | ------------------------- | ----------------------- |
| Search Engines | PageRank                  | Ranks web pages         |
| Maps           | Dijkstra‚Äôs Algorithm      | Finds shortest path     |
| Banking        | RSA Encryption            | Secure transactions     |
| E-commerce     | QuickSort + Binary Search | Sorting & searching     |
| AI/ML          | Backpropagation           | Neural network training |

Problem Type:
    1.Sorting 
    2.Searching   
    3.Optimization
    4.Graph Problem
    5.String Problem
    6.Numerical
    7.Geometric
    8.Combinatorial
    9.Divide and Conquer
    10.Dynamic Programming.

What is Algorithmic Efficiency?
    Algorithmic efficiency measures how well an algorithm uses resources when solving a problem, mainly:
        1.Time: How long the algorithm takes to run (running time)
                It‚Äôs usually expressed in terms of the number of basic operations or steps the algorithm performs.
        2.Space: How much memory it consumes (memory usage)

Types of Input Affecting Efficiency:
    1.Best Case: Input for which algorithm performs fastest
    2.Worst Case: Input for which algorithm performs slowest (important for guarantees)
    3.Average Case: Expected time over all inputs

 Asymptotic Notations:
   1.Big-O Notation O(g(n))     -> Upper bound (worst case)    -> f(n)‚â§c‚ãÖg(n) for some constant c and large n.
   2.Big-Omega Notation Œ©(g(n)) -> Lower bound (best case)	   -> f(n)‚â•c‚ãÖg(n) for some constant c, and large n.
   3.Big-Theta Notation Œò(g(n)) -> Tight bound (average case)  ->(n) is both O(g(n)) and Œ©(g(n)).

Non-recursive algorithms: Direct counting of operations
Recursive algorithms: Solving recurrence relations

Analysis of Non-Recursive Algorithms:
Steps:
    1.Identify basic operation
            The most important step the algorithm repeats (e.g., comparison, addition).
    2.Count frequency of basic operation
            How many times it executes in terms of n.
    3.Express as function of ùëõ
            Get T(n), the running time.
    4.Find asymptotic notation
            Simplify to O, Œ©,or Œò.
Example:                                   
    sum = 0;                                    
    for (i = 0; i < n; i++)
        sum += arr[i];

Analysis of Recursive Algorithms:
Steps:
    1.Form Recurrence Relation
        Write T(n) in terms of T(subproblem¬†size).
    2.Solve Recurrence
     We solve using:
        Substitution method
        Recursion tree
        Master theorem (quick formula for divide-and-conquer)
 
String Algorithms: Na√Øve String Matching Algorithm.
Example:
T = "AABAACAADAABAABA"
P = "AABA"
We want to find all positions where "AABA" occurs in T.

Naive String Matching Algorithm:
    1.Check the pattern at every possible position in the text.
    2.Compare characters of pattern P with substring of text T one by one.
    3.If all characters match ‚Üí pattern found.
    4.Otherwise ‚Üí shift by 1 and repeat.

Pseudocode:
NaivePatternSearch(T, P):
    n = length(T)
    m = length(P)
    for i = 0 to n - m:
        match = true
        for j = 0 to m - 1:
            if T[i + j] != P[j]:
                match = false
                break
        if match:
            print("Pattern found at index", i)

Example:
T = "AABAACAADAABAABA", P = "AABA"
i = 0 ‚Üí matches (output: index 0)
i = 1 ‚Üí mismatch (B vs A)
i = 2 ‚Üí mismatch
i = 3 ‚Üí mismatch
i = 9 ‚Üí matches (output: index 9)
i = 12 ‚Üí matches (output: index 12)
Output: Pattern found at indices 0, 9, 12            

Time Complexity:
    Worst case:O((n-m+1)m) ‚âà O(nm)
    Best case:O(n)
    Space complexity:O(1)
Space complexity:O(1)(no extra space needed)

Advantages:
    > Simple to implement
    > No preprocessing required

Disadvantages:
    > Inefficient for large text and patterns
    > Always shifts by 1, even when more can be skipped
    > Redundant Comparisons

Rabin-Karp Algorithm:
    The Rabin-Karp algorithm improves over the Na√Øve approach by using
    hashing to quickly compare substrings of the text with the pattern.
  Instead of comparing each character one-by-one, it:
    > Computes a hash value for the pattern.
    > Computes hash values for each substring of the text of the same length.
    > If hash values match ‚Üí do a direct character comparison
     (to confirm, avoiding false matches due to hash collisions).

Working process:
1.Preprocess:
    Let m = pattern length, n = text length.
    Compute hash of the pattern P.
    Compute hash of the first m characters of the text T.     

2.Slide the pattern over the text:
    At each position, compare the hash of the text window with the pattern‚Äôs hash.
    If hashes match ‚Üí compare actual characters (to avoid false positives).
    Update the hash for the next window using rolling hash.

3. Rolling Hash Concept
Instead of recomputing the hash from scratch for each window:
Remove the effect of the first character.
Add the effect of the new character.
This makes hash update O(1).     

Example(base d,mod q):
       hash(next)=(d(hash(current)-T[i].h)+T[i+m]) mod q
        where,
            h = d^m-1 mod q
            d = number of possible char
            q = a prime number(to  reduce collision)

Pseudocode:
RabinKarp(T, P, d, q):
    n = length(T)
    m = length(P)
    h = pow(d, m-1) % q
    p_hash = 0
    t_hash = 0

    // Precompute hash for pattern and first window
    for i in 0 to m-1:
        p_hash = (d * p_hash + P[i]) % q
        t_hash = (d * t_hash + T[i]) % q

    for i in 0 to n - m:
        if p_hash == t_hash:
            if T[i : i+m] == P:
                print("Pattern found at index", i)
        if i < n - m:
            t_hash = (d * (t_hash - T[i] * h) + T[i+m]) % q
            if t_hash < 0:
                t_hash += q

Example:
Example
T = "GEEKS FOR GEEKS", P = "GEEK", d = 256, q = 101
Compute pattern hash and first window hash.
Slide across text, updating hash in O(1).
When hashes match ‚Üí verify with direct comparison.

Complexity Analysis:
Average Case: O(n+m)
Worst Case:O(nm)
Space Complexity:O(1).

Advantages:
    > Fast for multiple pattern searches in the same text.
    > Efficient average-case performance.
    > Rolling hash avoids re-computing from scratch.

Disadvantage:
    > Hash collisions possible ‚Üí may cause false matches.
    > Worst-case performance same as Na√ØveO(nm).
    > Choosing a good modulus q is important to reduce collisions.

